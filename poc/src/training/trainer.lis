// PoC - Loop de Treinamento
// SGD com backpropagation completo para classificador de gestos

use paebiru::core::bytesil::{bs_mag, bs_from_mag_clamped, bs_zero};
use paebiru::core::state::{st_vacuum, st_set, st_get, st_add, st_sub, st_mul, st_scale, st_norm_l2, st_xor, st_tensor};
use paebiru::core::activations::{relu_state, softmax};
use paebiru::layers::dense::{dense_forward, dense_relu};
use data::synthetic::{generate_gesture_sample, generate_gesture_label, prng_next};

// =============================================================================
// Configuração de Treinamento
// =============================================================================

// Layout: [lr, momentum, epochs/1000, noise_level, clip_norm, step_size/100, gamma]
pub fn train_config(lr: Float, momentum: Float, epochs: Int, noise_level: Float) -> State {
    let config = st_vacuum();
    let config = st_set(config, 0, bs_from_mag_clamped(lr));
    let config = st_set(config, 1, bs_from_mag_clamped(momentum));
    let config = st_set(config, 2, bs_from_mag_clamped(float_from_int(epochs) / 1000.0));
    let config = st_set(config, 3, bs_from_mag_clamped(noise_level));
    let config = st_set(config, 4, bs_from_mag_clamped(1.0));  // clip_norm
    let config = st_set(config, 5, bs_from_mag_clamped(0.2));  // step_size = 20
    let config = st_set(config, 6, bs_from_mag_clamped(0.5));  // gamma
    return config;
}

pub fn get_lr(config: State) -> Float { return bs_mag(st_get(config, 0)); }
pub fn get_momentum(config: State) -> Float { return bs_mag(st_get(config, 1)); }
pub fn get_epochs(config: State) -> Int { return int_from_float(bs_mag(st_get(config, 2)) * 1000.0); }
pub fn get_noise(config: State) -> Float { return bs_mag(st_get(config, 3)); }

// =============================================================================
// Gradiente e Loss
// =============================================================================

// Gradiente da loss: grad = output - target (softmax + cross-entropy)
pub fn compute_output_gradient(output: State, target: State) -> State {
    return st_sub(output, target);
}

// Clip gradiente por norma L2
pub fn clip_gradient(grad: State, max_norm: Float) -> State {
    let norm = st_norm_l2(grad);
    if norm > max_norm {
        return st_scale(grad, max_norm / norm);
    }
    return grad;
}

// =============================================================================
// Backpropagation
// =============================================================================

// Backward através da camada 2 (linear)
// grad_output: gradiente vindo da loss
// Retorna: gradiente para hidden layer (grad_h1)
pub fn backward_layer2(grad_output: State, w2: State) -> State {
    // grad_h1 = grad_output * w2^T (simplificado: tensor product)
    return st_tensor(grad_output, w2);
}

// Backward através de ReLU
// grad: gradiente vindo da camada acima
// h1: saída da camada antes do ReLU
pub fn backward_relu(grad: State, h1: State) -> State {
    // ReLU derivative: 1 se h1 > 0, 0 caso contrário
    let result = st_vacuum();

    let g0 = bs_mag(st_get(grad, 0));
    let h0 = bs_mag(st_get(h1, 0));
    if h0 > 0.0 { let result = st_set(result, 0, bs_from_mag_clamped(g0)); }

    let g1 = bs_mag(st_get(grad, 1));
    let h1_val = bs_mag(st_get(h1, 1));
    if h1_val > 0.0 { let result = st_set(result, 1, bs_from_mag_clamped(g1)); }

    let g2 = bs_mag(st_get(grad, 2));
    let h2 = bs_mag(st_get(h1, 2));
    if h2 > 0.0 { let result = st_set(result, 2, bs_from_mag_clamped(g2)); }

    let g3 = bs_mag(st_get(grad, 3));
    let h3 = bs_mag(st_get(h1, 3));
    if h3 > 0.0 { let result = st_set(result, 3, bs_from_mag_clamped(g3)); }

    let g4 = bs_mag(st_get(grad, 4));
    let h4 = bs_mag(st_get(h1, 4));
    if h4 > 0.0 { let result = st_set(result, 4, bs_from_mag_clamped(g4)); }

    let g5 = bs_mag(st_get(grad, 5));
    let h5 = bs_mag(st_get(h1, 5));
    if h5 > 0.0 { let result = st_set(result, 5, bs_from_mag_clamped(g5)); }

    let g6 = bs_mag(st_get(grad, 6));
    let h6 = bs_mag(st_get(h1, 6));
    if h6 > 0.0 { let result = st_set(result, 6, bs_from_mag_clamped(g6)); }

    let g7 = bs_mag(st_get(grad, 7));
    let h7 = bs_mag(st_get(h1, 7));
    if h7 > 0.0 { let result = st_set(result, 7, bs_from_mag_clamped(g7)); }

    let g8 = bs_mag(st_get(grad, 8));
    let h8 = bs_mag(st_get(h1, 8));
    if h8 > 0.0 { let result = st_set(result, 8, bs_from_mag_clamped(g8)); }

    let g9 = bs_mag(st_get(grad, 9));
    let h9 = bs_mag(st_get(h1, 9));
    if h9 > 0.0 { let result = st_set(result, 9, bs_from_mag_clamped(g9)); }

    let g10 = bs_mag(st_get(grad, 10));
    let h10 = bs_mag(st_get(h1, 10));
    if h10 > 0.0 { let result = st_set(result, 10, bs_from_mag_clamped(g10)); }

    let g11 = bs_mag(st_get(grad, 11));
    let h11 = bs_mag(st_get(h1, 11));
    if h11 > 0.0 { let result = st_set(result, 11, bs_from_mag_clamped(g11)); }

    let g12 = bs_mag(st_get(grad, 12));
    let h12 = bs_mag(st_get(h1, 12));
    if h12 > 0.0 { let result = st_set(result, 12, bs_from_mag_clamped(g12)); }

    let g13 = bs_mag(st_get(grad, 13));
    let h13 = bs_mag(st_get(h1, 13));
    if h13 > 0.0 { let result = st_set(result, 13, bs_from_mag_clamped(g13)); }

    let g14 = bs_mag(st_get(grad, 14));
    let h14 = bs_mag(st_get(h1, 14));
    if h14 > 0.0 { let result = st_set(result, 14, bs_from_mag_clamped(g14)); }

    let g15 = bs_mag(st_get(grad, 15));
    let h15 = bs_mag(st_get(h1, 15));
    if h15 > 0.0 { let result = st_set(result, 15, bs_from_mag_clamped(g15)); }

    return result;
}

// =============================================================================
// SGD Updates
// =============================================================================

// SGD básico: weights = weights - lr * gradient
pub fn sgd_update(weights: State, gradient: State, lr: Float) -> State {
    return st_sub(weights, st_scale(gradient, lr));
}

// SGD com momentum
// velocity_new = momentum * velocity + gradient
// weights_new = weights - lr * velocity_new
// Retorna: (new_weights, new_velocity) empacotados via XOR para referência futura
pub fn sgd_momentum_update(
    weights: State,
    gradient: State,
    velocity: State,
    lr: Float,
    momentum: Float
) -> State {
    let new_velocity = st_add(st_scale(velocity, momentum), gradient);
    return st_sub(weights, st_scale(new_velocity, lr));
}

// Retorna nova velocity para próxima iteração
pub fn compute_velocity(velocity: State, gradient: State, momentum: Float) -> State {
    return st_add(st_scale(velocity, momentum), gradient);
}

// =============================================================================
// Training Step Completo (Backprop em todas as camadas)
// =============================================================================

// Forward pass com cache das ativações intermediárias
// Retorna: output (probabilidades softmax)
pub fn forward_with_cache(
    input: State,
    w1: State,
    b1: State,
    w2: State,
    b2: State
) -> State {
    // Layer 1: linear
    let z1 = dense_forward(input, w1, b1);
    // Layer 1: ReLU (a saída após ReLU é h1)
    let h1 = relu_state(z1);
    // Layer 2: linear
    let z2 = dense_forward(h1, w2, b2);
    // Softmax
    return softmax(z2);
}

// Retorna hidden activations (h1) para backprop
pub fn get_hidden_activation(input: State, w1: State, b1: State) -> State {
    let z1 = dense_forward(input, w1, b1);
    return relu_state(z1);
}

// Retorna z1 (antes do ReLU) para backward
pub fn get_pre_activation(input: State, w1: State, b1: State) -> State {
    return dense_forward(input, w1, b1);
}

// Training step completo com backprop em TODAS as camadas
// Retorna State empacotado com flags indicando sucesso
// Os pesos são passados por referência e modificados in-place
pub fn train_step_full(
    input: State,
    label: State,
    w1: State,
    b1: State,
    w2: State,
    b2: State,
    v_w1: State,
    v_b1: State,
    v_w2: State,
    v_b2: State,
    lr: Float,
    momentum: Float
) -> State {
    // ===== FORWARD PASS =====
    // Layer 1
    let z1 = dense_forward(input, w1, b1);
    let h1 = relu_state(z1);

    // Layer 2
    let z2 = dense_forward(h1, w2, b2);
    let output = softmax(z2);

    // ===== BACKWARD PASS =====
    // Gradiente da loss (softmax + cross-entropy simplificado)
    let grad_output = st_sub(output, label);
    let grad_output = clip_gradient(grad_output, 1.0);

    // Gradiente para w2 e b2
    // grad_w2 = h1^T * grad_output (outer product simplificado)
    let grad_w2 = st_tensor(h1, grad_output);
    let grad_b2 = grad_output;

    // Backprop através da camada 2
    let grad_h1 = backward_layer2(grad_output, w2);

    // Backprop através de ReLU
    let grad_z1 = backward_relu(grad_h1, z1);

    // Gradiente para w1 e b1
    let grad_w1 = st_tensor(input, grad_z1);
    let grad_b1 = grad_z1;

    // ===== UPDATE WEIGHTS com MOMENTUM =====
    // Update w1
    let v_w1_new = compute_velocity(v_w1, grad_w1, momentum);
    let w1_new = sgd_momentum_update(w1, grad_w1, v_w1, lr, momentum);

    // Update b1
    let v_b1_new = compute_velocity(v_b1, grad_b1, momentum);
    let b1_new = sgd_momentum_update(b1, grad_b1, v_b1, lr, momentum);

    // Update w2
    let v_w2_new = compute_velocity(v_w2, grad_w2, momentum);
    let w2_new = sgd_momentum_update(w2, grad_w2, v_w2, lr, momentum);

    // Update b2
    let v_b2_new = compute_velocity(v_b2, grad_b2, momentum);
    let b2_new = sgd_momentum_update(b2, grad_b2, v_b2, lr, momentum);

    // Empacota os novos pesos em um State de retorno
    // (Usamos XOR chain para combinar informações de status)
    let status = st_xor(w1_new, b1_new);
    let status = st_xor(status, w2_new);
    let status = st_xor(status, b2_new);

    return status;
}

// =============================================================================
// Training Epoch com Todos os Pesos
// =============================================================================

// Estrutura simplificada: treina e retorna tupla de estados
// Como LIS não tem tuples, retornamos w1 atualizado e usamos variáveis globais
// para outros pesos (simplificação para PoC)

// Treina uma época completa - versão que atualiza todos os pesos
pub fn train_epoch_full(
    w1: State, b1: State, w2: State, b2: State,
    v_w1: State, v_b1: State, v_w2: State, v_b2: State,
    num_samples: Int,
    lr: Float,
    momentum: Float,
    noise_level: Float,
    seed: Int
) -> State {
    let cw1 = w1;
    let cb1 = b1;
    let cw2 = w2;
    let cb2 = b2;
    let cv_w1 = v_w1;
    let cv_b1 = v_b1;
    let cv_w2 = v_w2;
    let cv_b2 = v_b2;
    let s = seed;

    let i = 0;
    loop {
        if i >= num_samples { break; }

        // Seleciona classe aleatória
        let s = prng_next(s);
        let gesture = s % 16;

        // Gera sample e label
        let s = prng_next(s);
        let input = generate_gesture_sample(gesture, s, noise_level);
        let label = generate_gesture_label(gesture);

        // ===== FORWARD =====
        let z1 = dense_forward(input, cw1, cb1);
        let h1 = relu_state(z1);
        let z2 = dense_forward(h1, cw2, cb2);
        let output = softmax(z2);

        // ===== BACKWARD =====
        let grad_out = clip_gradient(st_sub(output, label), 1.0);

        // Gradientes camada 2
        let grad_w2 = st_tensor(h1, grad_out);
        let grad_b2 = grad_out;

        // Backprop para camada 1
        let grad_h1 = backward_layer2(grad_out, cw2);
        let grad_z1 = backward_relu(grad_h1, z1);
        let grad_w1 = st_tensor(input, grad_z1);
        let grad_b1 = grad_z1;

        // ===== UPDATE com MOMENTUM =====
        let cv_w1 = st_add(st_scale(cv_w1, momentum), grad_w1);
        let cw1 = st_sub(cw1, st_scale(cv_w1, lr));

        let cv_b1 = st_add(st_scale(cv_b1, momentum), grad_b1);
        let cb1 = st_sub(cb1, st_scale(cv_b1, lr));

        let cv_w2 = st_add(st_scale(cv_w2, momentum), grad_w2);
        let cw2 = st_sub(cw2, st_scale(cv_w2, lr));

        let cv_b2 = st_add(st_scale(cv_b2, momentum), grad_b2);
        let cb2 = st_sub(cb2, st_scale(cv_b2, lr));

        let i = i + 1;
    }

    // Retorna w1 (os outros pesos precisam ser acessados de outra forma)
    // Para PoC, empacotamos informação de convergência
    return cw1;
}

// =============================================================================
// Training Loop Completo com LR Scheduling
// =============================================================================

pub fn train_full(
    w1: State, b1: State, w2: State, b2: State,
    num_epochs: Int,
    samples_per_epoch: Int,
    initial_lr: Float,
    momentum: Float,
    noise_level: Float,
    seed: Int
) -> State {
    // Inicializa velocities para momentum
    let v_w1 = st_vacuum();
    let v_b1 = st_vacuum();
    let v_w2 = st_vacuum();
    let v_b2 = st_vacuum();

    let cw1 = w1;
    let cb1 = b1;
    let cw2 = w2;
    let cb2 = b2;
    let s = seed;

    let epoch = 0;
    loop {
        if epoch >= num_epochs { break; }

        // Learning rate com decay
        let lr = lr_decay_step(initial_lr, epoch, 20, 0.5);

        // Treina uma época
        let s = prng_next(s);

        let i = 0;
        loop {
            if i >= samples_per_epoch { break; }

            let s = prng_next(s);
            let gesture = s % 16;
            let s = prng_next(s);
            let input = generate_gesture_sample(gesture, s, noise_level);
            let label = generate_gesture_label(gesture);

            // Forward
            let z1 = dense_forward(input, cw1, cb1);
            let h1 = relu_state(z1);
            let z2 = dense_forward(h1, cw2, cb2);
            let output = softmax(z2);

            // Backward
            let grad_out = clip_gradient(st_sub(output, label), 1.0);
            let grad_w2 = st_tensor(h1, grad_out);
            let grad_b2 = grad_out;
            let grad_h1 = backward_layer2(grad_out, cw2);
            let grad_z1 = backward_relu(grad_h1, z1);
            let grad_w1 = st_tensor(input, grad_z1);
            let grad_b1 = grad_z1;

            // Update ALL weights
            let v_w1 = st_add(st_scale(v_w1, momentum), grad_w1);
            let cw1 = st_sub(cw1, st_scale(v_w1, lr));

            let v_b1 = st_add(st_scale(v_b1, momentum), grad_b1);
            let cb1 = st_sub(cb1, st_scale(v_b1, lr));

            let v_w2 = st_add(st_scale(v_w2, momentum), grad_w2);
            let cw2 = st_sub(cw2, st_scale(v_w2, lr));

            let v_b2 = st_add(st_scale(v_b2, momentum), grad_b2);
            let cb2 = st_sub(cb2, st_scale(v_b2, lr));

            let i = i + 1;
        }

        let epoch = epoch + 1;
    }

    return cw1;
}

// =============================================================================
// Learning Rate Scheduling
// =============================================================================

pub fn lr_decay_linear(initial_lr: Float, epoch: Int, total_epochs: Int) -> Float {
    let progress = float_from_int(epoch) / float_from_int(total_epochs);
    return initial_lr * (1.0 - progress * 0.9);  // Decai até 10% do inicial
}

pub fn lr_decay_step(initial_lr: Float, epoch: Int, step_size: Int, gamma: Float) -> Float {
    let num_steps = epoch / step_size;
    let decay = 1.0;
    let i = 0;
    loop {
        if i >= num_steps { break; }
        let decay = decay * gamma;
        let i = i + 1;
    }
    return initial_lr * decay;
}

// =============================================================================
// Legacy API (compatibilidade)
// =============================================================================

pub fn train_step(input: State, label: State, w1: State, b1: State, w2: State, b2: State, lr: Float) -> State {
    let z1 = dense_forward(input, w1, b1);
    let h1 = relu_state(z1);
    let z2 = dense_forward(h1, w2, b2);
    let output = softmax(z2);
    let grad = clip_gradient(st_sub(output, label), 1.0);
    return sgd_update(w1, st_tensor(input, backward_relu(backward_layer2(grad, w2), z1)), lr);
}

pub fn train_epoch(w1: State, b1: State, w2: State, b2: State, num_samples: Int, lr: Float, noise_level: Float, seed: Int) -> State {
    return train_epoch_full(w1, b1, w2, b2, st_vacuum(), st_vacuum(), st_vacuum(), st_vacuum(), num_samples, lr, 0.0, noise_level, seed);
}

pub fn train(w1: State, b1: State, w2: State, b2: State, num_epochs: Int, samples_per_epoch: Int, lr: Float, noise_level: Float, seed: Int) -> State {
    return train_full(w1, b1, w2, b2, num_epochs, samples_per_epoch, lr, 0.9, noise_level, seed);
}
