// LIS Example: Otimizador SGD
// Demonstra: SGD básico e SGD com momentum
// Documentação: Seção 12.7

// =========================================
// SGD (Stochastic Gradient Descent)
// =========================================
// Atualização: params = params - lr * gradients
// Com momentum: v = momentum * v - lr * gradients
//               params = params + v

// =========================================
// Funções Auxiliares
// =========================================

fn scale_state(s: State, factor: Float) -> State {
    return transform_magnitude_scale(s, factor);
}

fn subtract_states(a: State, b: State) -> State {
    // Aproximação de subtração via XOR
    return state_xor(a, b);
}

fn add_states(a: State, b: State) -> State {
    // Aproximação de soma via XOR
    return state_xor(a, b);
}

// =========================================
// SGD Básico
// =========================================

fn sgd_step(params: State, gradients: State, lr: Float) -> State {
    // params = params - lr * gradients
    let scaled_grad = scale_state(gradients, lr);
    return subtract_states(params, scaled_grad);
}

fn sgd_update_multiple(params: State, gradients: State, lr: Float, steps: Int) -> State {
    let current = params;
    let i = 0;

    loop {
        if i >= steps {
            break;
        }

        let current = sgd_step(current, gradients, lr);
        let i = i + 1;
    }

    return current;
}

// =========================================
// SGD com Momentum
// =========================================

fn sgd_momentum_step(params: State, gradients: State, velocity: State, lr: Float, momentum: Float) -> State {
    // v = momentum * v - lr * gradients
    let v_momentum = scale_state(velocity, momentum);
    let grad_scaled = scale_state(gradients, lr);
    let new_velocity = subtract_states(v_momentum, grad_scaled);

    // params = params + v
    return add_states(params, new_velocity);
}

// Retorna novo velocity (para uso em próxima iteração)
fn compute_velocity(velocity: State, gradients: State, lr: Float, momentum: Float) -> State {
    let v_momentum = scale_state(velocity, momentum);
    let grad_scaled = scale_state(gradients, lr);
    return subtract_states(v_momentum, grad_scaled);
}

// =========================================
// SGD com Nesterov Momentum
// =========================================

fn sgd_nesterov_step(params: State, gradients: State, velocity: State, lr: Float, momentum: Float) -> State {
    // Nesterov: look ahead position first
    let look_ahead = add_states(params, scale_state(velocity, momentum));

    // Compute gradient at look ahead (simplificado: usar gradients passados)
    let grad_scaled = scale_state(gradients, lr);

    // Update velocity
    let new_velocity = subtract_states(scale_state(velocity, momentum), grad_scaled);

    // Update params
    return add_states(params, new_velocity);
}

// =========================================
// SGD com Weight Decay (L2 Regularization)
// =========================================

fn sgd_weight_decay_step(params: State, gradients: State, lr: Float, weight_decay: Float) -> State {
    // gradients_with_decay = gradients + weight_decay * params
    let decay_term = scale_state(params, weight_decay);
    let total_grad = add_states(gradients, decay_term);

    // Standard SGD update
    let scaled_grad = scale_state(total_grad, lr);
    return subtract_states(params, scaled_grad);
}

// =========================================
// Batch SGD
// =========================================

fn sgd_batch_step(params: State, batch_gradients: State, lr: Float, batch_size: Int) -> State {
    // Média dos gradientes do batch
    let avg_factor = 1.0 / batch_size * 1.0;
    let avg_gradients = scale_state(batch_gradients, avg_factor);

    // Standard SGD update
    return sgd_step(params, avg_gradients, lr);
}

// =========================================
// Learning Rate Decay
// =========================================

fn sgd_with_lr_decay(params: State, gradients: State, initial_lr: Float, decay_rate: Float, epoch: Int) -> State {
    // lr = initial_lr / (1 + decay_rate * epoch)
    let lr = initial_lr / (1.0 + decay_rate * epoch * 1.0);
    return sgd_step(params, gradients, lr);
}

// =========================================
// Gradient Clipping
// =========================================

fn clip_gradients(gradients: State, max_norm: Float) -> State {
    // Calcular norma total dos gradientes
    let norm_sq = 0.0;
    let idx = 0;

    loop {
        if idx >= 16 {
            break;
        }
        let mag = bytesil_magnitude(state_get_layer(gradients, idx));
        let norm_sq = norm_sq + mag * mag;
        let idx = idx + 1;
    }

    let norm = sqrt(norm_sq);

    // Se norma > max_norm, escalar para baixo
    if norm > max_norm {
        let scale_factor = max_norm / norm;
        return scale_state(gradients, scale_factor);
    }

    return gradients;
}

fn sgd_with_clipping(params: State, gradients: State, lr: Float, max_grad_norm: Float) -> State {
    let clipped = clip_gradients(gradients, max_grad_norm);
    return sgd_step(params, clipped, lr);
}

fn main() {
    print_string("=== SGD Optimizer Demo ===");

    // =========================================
    // Setup
    // =========================================
    print_string("--- Setup ---");

    let params = state_neutral();
    print_string("Initial params L0 magnitude:");
    print_float(bytesil_magnitude(state_get_layer(params, 0)));

    // Simular gradientes
    let gradients = state_vacuum();
    let gradients = state_set_layer(gradients, 0, bytesil_new(5, 0));
    let gradients = state_set_layer(gradients, 1, bytesil_new(3, 4));
    print_string("Gradients created");

    let lr = 0.1;
    print_string("Learning rate: 0.1");

    // =========================================
    // SGD Básico
    // =========================================
    print_string("--- Basic SGD ---");

    let updated = sgd_step(params, gradients, lr);
    print_string("After 1 SGD step, L0 magnitude:");
    print_float(bytesil_magnitude(state_get_layer(updated, 0)));

    let updated_5 = sgd_update_multiple(params, gradients, lr, 5);
    print_string("After 5 SGD steps, L0 magnitude:");
    print_float(bytesil_magnitude(state_get_layer(updated_5, 0)));

    // =========================================
    // SGD com Momentum
    // =========================================
    print_string("--- SGD with Momentum ---");

    let velocity = state_vacuum();
    let momentum = 0.9;

    let momentum_updated = sgd_momentum_step(params, gradients, velocity, lr, momentum);
    print_string("After momentum SGD step, L0 magnitude:");
    print_float(bytesil_magnitude(state_get_layer(momentum_updated, 0)));

    // =========================================
    // SGD com Weight Decay
    // =========================================
    print_string("--- SGD with Weight Decay ---");

    let weight_decay = 0.01;
    let wd_updated = sgd_weight_decay_step(params, gradients, lr, weight_decay);
    print_string("After SGD + weight decay, L0 magnitude:");
    print_float(bytesil_magnitude(state_get_layer(wd_updated, 0)));

    // =========================================
    // Gradient Clipping
    // =========================================
    print_string("--- Gradient Clipping ---");

    let large_grad = state_vacuum();
    let large_grad = state_set_layer(large_grad, 0, bytesil_new(15, 0));  // Large gradient

    let clipped = clip_gradients(large_grad, 1.0);
    print_string("Original gradient L0 magnitude:");
    print_float(bytesil_magnitude(state_get_layer(large_grad, 0)));
    print_string("Clipped gradient L0 magnitude:");
    print_float(bytesil_magnitude(state_get_layer(clipped, 0)));

    // =========================================
    // Training Loop Simulation
    // =========================================
    print_string("--- Training Loop (10 epochs) ---");

    let train_params = state_neutral();
    let epoch = 0;

    loop {
        if epoch >= 10 {
            break;
        }

        // Simular gradientes diferentes a cada época
        let epoch_grad = scale_state(gradients, 1.0 / (epoch * 1.0 + 1.0));

        // SGD com LR decay
        let train_params = sgd_with_lr_decay(train_params, epoch_grad, 0.1, 0.1, epoch);

        let epoch = epoch + 1;
    }

    print_string("After 10 epochs, L0 magnitude:");
    print_float(bytesil_magnitude(state_get_layer(train_params, 0)));

    print_string("=== SGD Optimizer Demo Complete ===");
}
