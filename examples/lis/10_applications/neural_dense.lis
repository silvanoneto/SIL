// LIS Example: Camada Dense de Rede Neural
// Demonstra: Implementação de camada fully connected
// Documentação: Seção 12.6

// =========================================
// Dense Layer: output = activation(input * weights + bias)
// =========================================
// State como vetor de 16 elementos
// Operações: tensor product para multiplicação, xor para adição

// =========================================
// Forward Pass Básico
// =========================================

fn dense_forward(input: State, weights: State, bias: State) -> State {
    // Computar soma ponderada
    let weighted = state_tensor(input, weights);

    // Adicionar bias
    let with_bias = state_xor(weighted, bias);

    return with_bias;
}

// =========================================
// Dense sem Bias
// =========================================

fn dense_no_bias(input: State, weights: State) -> State {
    return state_tensor(input, weights);
}

// =========================================
// Dense com Ativações
// =========================================

fn dense_relu(input: State, weights: State, bias: State) -> State {
    let linear = dense_forward(input, weights, bias);
    return relu_state(linear);
}

fn dense_sigmoid(input: State, weights: State, bias: State) -> State {
    let linear = dense_forward(input, weights, bias);
    return sigmoid_state_impl(linear);
}

// Sigmoid implementation for State
fn sigmoid_state_impl(s: State) -> State {
    let result = state_vacuum();
    let idx = 0;

    loop {
        if idx >= 16 {
            break;
        }

        let layer = state_get_layer(s, idx);
        let mag = bytesil_magnitude(layer);
        let sig = 1.0 / (1.0 + exp(0.0 - mag));
        let new_layer = bytesil_from_complex(sig, 0.0);
        let result = state_set_layer(result, idx, new_layer);

        let idx = idx + 1;
    }

    return result;
}

// =========================================
// Inicialização de Pesos
// =========================================

fn init_weights_random(seed: Float) -> State {
    let result = state_vacuum();
    let base = seed - floor(seed / 16.0) * 16.0;
    let idx = 0;

    loop {
        if idx >= 16 {
            break;
        }

        let theta = base + idx * 1.0;
        let theta = theta - floor(theta / 16.0) * 16.0;
        let w = bytesil_new(1, floor(theta));  // Pequena magnitude
        let result = state_set_layer(result, idx, w);

        let idx = idx + 1;
    }

    return result;
}

fn init_bias_zero() -> State {
    return state_vacuum();
}

fn init_weights_uniform(value: ByteSil) -> State {
    let result = state_vacuum();
    let idx = 0;

    loop {
        if idx >= 16 {
            break;
        }
        let result = state_set_layer(result, idx, value);
        let idx = idx + 1;
    }

    return result;
}

// =========================================
// Multi-Layer Perceptron (MLP)
// =========================================

fn mlp_2_layers(input: State, w1: State, b1: State, w2: State, b2: State) -> State {
    // Primeira camada com ReLU
    let h1 = dense_relu(input, w1, b1);

    // Segunda camada (output)
    let output = dense_forward(h1, w2, b2);

    return output;
}

fn mlp_3_layers(input: State, w1: State, b1: State, w2: State, b2: State, w3: State, b3: State) -> State {
    // Primeira camada oculta
    let h1 = dense_relu(input, w1, b1);

    // Segunda camada oculta
    let h2 = dense_relu(h1, w2, b2);

    // Camada de saída
    let output = dense_forward(h2, w3, b3);

    return output;
}

// =========================================
// Normalização de Camada
// =========================================

fn layer_norm(s: State) -> State {
    // Calcular média das magnitudes
    let sum = 0.0;
    let idx = 0;

    loop {
        if idx >= 16 {
            break;
        }
        let sum = sum + bytesil_magnitude(state_get_layer(s, idx));
        let idx = idx + 1;
    }

    let mean = sum / 16.0;

    // Calcular variância
    let var_sum = 0.0;
    let idx2 = 0;

    loop {
        if idx2 >= 16 {
            break;
        }
        let diff = bytesil_magnitude(state_get_layer(s, idx2)) - mean;
        let var_sum = var_sum + diff * diff;
        let idx2 = idx2 + 1;
    }

    let variance = var_sum / 16.0;
    let std_dev = sqrt(variance + 0.00001);  // Evitar divisão por zero

    // Normalizar cada camada
    let result = state_vacuum();
    let idx3 = 0;

    loop {
        if idx3 >= 16 {
            break;
        }
        let layer = state_get_layer(s, idx3);
        let mag = bytesil_magnitude(layer);
        let norm_mag = (mag - mean) / std_dev;
        let new_layer = bytesil_from_complex(norm_mag, bytesil_phase_radians(layer));
        let result = state_set_layer(result, idx3, new_layer);
        let idx3 = idx3 + 1;
    }

    return result;
}

// =========================================
// Pipeline Transforms (moved to top-level)
// =========================================

transform dense_transform(s: State) -> State {
    let w = init_weights_random(7.0);
    let b = init_bias_zero();
    return dense_relu(s, w, b);
}

transform norm_transform(s: State) -> State {
    return layer_norm(s);
}

// =========================================
// Main
// =========================================

fn main() {
    print_string("=== Neural Dense Layer Demo ===");

    // =========================================
    // Criar dados de teste
    // =========================================
    print_string("--- Creating test data ---");

    let input = state_neutral();
    print_string("Input created (neutral state)");

    let weights = init_weights_random(42.0);
    print_string("Weights initialized (random seed 42)");

    let bias = init_bias_zero();
    print_string("Bias initialized (zeros)");

    // =========================================
    // Forward Pass
    // =========================================
    print_string("--- Forward Pass ---");

    let output = dense_forward(input, weights, bias);
    print_string("Dense forward complete");
    print_string("Output L0 magnitude:");
    print_float(bytesil_magnitude(state_get_layer(output, 0)));

    // =========================================
    // Com ReLU
    // =========================================
    print_string("--- Dense + ReLU ---");

    let output_relu = dense_relu(input, weights, bias);
    print_string("Dense with ReLU complete");
    print_string("Output L0 magnitude:");
    print_float(bytesil_magnitude(state_get_layer(output_relu, 0)));

    // =========================================
    // MLP de 2 Camadas
    // =========================================
    print_string("--- 2-Layer MLP ---");

    let w1 = init_weights_random(1.0);
    let b1 = init_bias_zero();
    let w2 = init_weights_random(2.0);
    let b2 = init_bias_zero();

    let mlp_output = mlp_2_layers(input, w1, b1, w2, b2);
    print_string("2-layer MLP forward complete");
    print_string("Output L0 magnitude:");
    print_float(bytesil_magnitude(state_get_layer(mlp_output, 0)));

    // =========================================
    // Layer Normalization
    // =========================================
    print_string("--- Layer Normalization ---");

    let normalized = layer_norm(output);
    print_string("Layer norm applied");
    print_string("Normalized L0 magnitude:");
    print_float(bytesil_magnitude(state_get_layer(normalized, 0)));

    // =========================================
    // Pipeline Completo
    // =========================================
    print_string("--- Complete Pipeline ---");

    let pipeline_output = input
        |> dense_transform
        |> norm_transform
        |> dense_transform
        |> norm_transform;

    print_string("Complete pipeline output L0:");
    print_float(bytesil_magnitude(state_get_layer(pipeline_output, 0)));

    print_string("=== Neural Dense Demo Complete ===");
}
