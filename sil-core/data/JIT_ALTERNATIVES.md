# Alternativas de JIT para VSP

## ğŸ“Š AnÃ¡lise Comparativa

Atualmente usamos **Cranelift** para AOT. Para JIT, temos vÃ¡rias opÃ§Ãµes:

---

## 1. ğŸ¦€ **Cranelift JIT** (Atual no feature gate)

### âœ… PrÃ³s
- **JÃ¡ integrado**: Usamos Cranelift para AOT
- **Rust-native**: Zero-cost abstractions, seguranÃ§a de memÃ³ria
- **Compilation rÃ¡pida**: ~1-5ms por funÃ§Ã£o
- **Tier do Wasmtime**: ProduÃ§Ã£o-ready
- **Multi-platform**: x86_64, ARM64, RISC-V
- **Ã“timo documentation**: https://cranelift.dev

### âŒ Contras
- **Performance JIT**: ~3-5x mais lento que LLVM JIT
- **CÃ³digo emitido**: Menos otimizado que LLVM
- **Overhead**: ~500KB de runtime

### ğŸ“ Use Case
- **Ideal para**: Compilation rÃ¡pida, startup baixo, embedded
- **Evitar**: Quando precisa mÃ¡xima performance de execuÃ§Ã£o

### ğŸ’» ImplementaÃ§Ã£o Estimada
```rust
use cranelift_jit::{JITBuilder, JITModule};

pub struct VspJit {
    module: JITModule,
    functions: HashMap<String, *const u8>,
}

impl VspJit {
    pub fn compile(&mut self, bytecode: &SilcFile) -> VspResult<()> {
        // Similar ao AOT, mas com JITModule
        let mut ctx = self.module.make_context();
        // ... build IR
        let id = self.module.declare_function("main", Linkage::Export, &sig)?;
        self.module.define_function(id, &mut ctx)?;
        self.module.finalize_definitions()?;
        
        // Get pointer
        let ptr = self.module.get_finalized_function(id);
        self.functions.insert("main".into(), ptr);
        Ok(())
    }
    
    pub fn execute(&self, name: &str, state: &mut SilState) -> i32 {
        let func: fn(*mut SilState) -> i32 = unsafe {
            std::mem::transmute(self.functions[name])
        };
        func(state)
    }
}
```

**Tempo de implementaÃ§Ã£o**: 2-3 dias

---

## 2. ğŸ”¥ **LLVM JIT** (via inkwell)

### âœ… PrÃ³s
- **Performance mÃ¡xima**: CÃ³digo ~2x mais rÃ¡pido que Cranelift
- **OtimizaÃ§Ãµes agressivas**: Inlining, loop unrolling, vectorizaÃ§Ã£o
- **Industry standard**: Usado por Julia, Python (Numba), Rust (rustc)
- **Debug info**: DWARF completo para debugging

### âŒ Contras
- **Compilation lenta**: ~50-100ms por funÃ§Ã£o (10-20x mais lento)
- **Overhead gigante**: ~50-100MB de runtime
- **Complexidade**: API verbosa, lifetimes difÃ­ceis
- **Build time**: LLVM demora para compilar

### ğŸ“ Use Case
- **Ideal para**: Long-running computations, HPC, quando tempo de compile nÃ£o importa
- **Evitar**: AplicaÃ§Ãµes interativas, low-latency, embedded

### ğŸ’» ImplementaÃ§Ã£o Estimada
```rust
use inkwell::context::Context;
use inkwell::execution_engine::ExecutionEngine;

pub struct LlvmJit<'ctx> {
    context: &'ctx Context,
    engine: ExecutionEngine<'ctx>,
}

impl<'ctx> LlvmJit<'ctx> {
    pub fn compile(&mut self, bytecode: &SilcFile) -> VspResult<()> {
        let module = self.context.create_module("vsp");
        let builder = self.context.create_builder();
        
        // Define function signature
        let i64_type = self.context.i64_type();
        let fn_type = i64_type.fn_type(&[i64_type.into()], false);
        let function = module.add_function("main", fn_type, None);
        
        // Build IR
        let entry = self.context.append_basic_block(function, "entry");
        builder.position_at_end(entry);
        // ... translate opcodes
        
        self.engine.add_module(&module)?;
        Ok(())
    }
}
```

**Tempo de implementaÃ§Ã£o**: 1-2 semanas (API complexa)

**DependÃªncia**: 
```toml
inkwell = { version = "0.5", features = ["llvm18-0"] }
```

---

## 3. âš¡ **LuaJIT-style Tracing JIT**

### ğŸ’¡ Conceito
NÃ£o compila bytecode diretamente. Interpreta primeiro, detecta **hot loops**, compila apenas o caminho quente.

### âœ… PrÃ³s
- **Startup zero**: Interpretador puro no inÃ­cio
- **Performance excelente**: Em hot paths, ~5-10x mais rÃ¡pido
- **Memory efficient**: SÃ³ compila o que usa
- **Adaptive**: Re-compila com novos tipos/valores

### âŒ Contras
- **Complexidade altÃ­ssima**: Precisa implementar tracer completo
- **Profile-guided**: Performance varia com workload
- **Debugging difÃ­cil**: CÃ³digo hÃ­brido interpretado/compilado

### ğŸ“ Use Case
- **Ideal para**: AplicaÃ§Ãµes com loops quentes claros (ML training, games)
- **Evitar**: CÃ³digo linear, execuÃ§Ãµes Ãºnicas

### ğŸ’» ImplementaÃ§Ã£o Estimada
```rust
pub struct TracingJit {
    interpreter: VspInterpreter,
    hot_spots: HashMap<u32, HotSpot>, // PC -> contador
    compiled: HashMap<u32, CompiledTrace>,
    threshold: u32, // execuÃ§Ãµes antes de compilar
}

impl TracingJit {
    pub fn execute(&mut self, state: &mut SilState) -> VspResult<i32> {
        loop {
            let pc = state.pc;
            
            // Check se tem versÃ£o compilada
            if let Some(trace) = self.compiled.get(&pc) {
                trace.execute(state)?;
                continue;
            }
            
            // Interpretar e contar
            self.interpreter.step(state)?;
            
            // Detectar hot spot
            let count = self.hot_spots.entry(pc).or_insert(0);
            *count += 1;
            
            if *count >= self.threshold {
                // Compilar trace!
                self.compile_trace(pc)?;
            }
        }
    }
    
    fn compile_trace(&mut self, start_pc: u32) -> VspResult<()> {
        // Record trace execution
        let trace = self.record_trace(start_pc)?;
        // Compile to native
        let compiled = cranelift_compile(trace)?;
        self.compiled.insert(start_pc, compiled);
        Ok(())
    }
}
```

**Tempo de implementaÃ§Ã£o**: 3-4 semanas (muito complexo)

---

## 4. ğŸ¯ **DynASM** (Runtime Assembly)

### ğŸ’¡ Conceito
Gera cÃ³digo assembly diretamente em runtime, sem IR intermediÃ¡rio.

### âœ… PrÃ³s
- **Compilation ultra-rÃ¡pida**: ~0.1-0.5ms por funÃ§Ã£o
- **Overhead mÃ­nimo**: ~50KB runtime
- **Performance Ã³tima**: CÃ³digo manual assembly
- **Controle total**: Sem abstraÃ§Ãµes

### âŒ Contras
- **Portabilidade zero**: Precisa escrever para x86_64, ARM64, etc separadamente
- **Unsafe**: Muito cÃ³digo `unsafe`, fÃ¡cil quebrar
- **ManutenÃ§Ã£o**: Assembly Ã© difÃ­cil de manter
- **Debug**: Sem stack traces, sem DWARF

### ğŸ“ Use Case
- **Ideal para**: Quando precisa de compilation instantÃ¢nea + performance mÃ¡xima
- **Evitar**: Se portabilidade importa, se equipe nÃ£o sabe assembly

### ğŸ’» ImplementaÃ§Ã£o Estimada
```rust
use dynasmrt::{dynasm, DynasmApi, DynasmLabelApi};

pub struct DynasmJit {
    ops: dynasmrt::x64::Assembler,
}

impl DynasmJit {
    pub fn compile(&mut self, bytecode: &SilcFile) -> VspResult<*const u8> {
        dynasm!(self.ops
            ; .arch x64
            ; ->main:
            ; push rbp
            ; mov rbp, rsp
        );
        
        for inst in &bytecode.instructions {
            match inst.opcode {
                Opcode::Nop => {
                    dynasm!(self.ops
                        ; nop
                    );
                }
                Opcode::Add { dst, src } => {
                    dynasm!(self.ops
                        ; mov rax, [rdi + (dst * 8) as i32]
                        ; add rax, [rdi + (src * 8) as i32]
                        ; mov [rdi + (dst * 8) as i32], rax
                    );
                }
                // ... outros opcodes
            }
        }
        
        dynasm!(self.ops
            ; xor rax, rax
            ; pop rbp
            ; ret
        );
        
        let buf = self.ops.finalize().unwrap();
        Ok(buf.ptr(AssemblyOffset(0)))
    }
}
```

**Tempo de implementaÃ§Ã£o**: 2-3 semanas (assembly para cada opcode)

**DependÃªncia**:
```toml
dynasmrt = "2.0"
```

---

## 5. ğŸŒ **WebAssembly (Wasmtime)**

### ğŸ’¡ Conceito
Compila bytecode VSP â†’ WASM â†’ JIT execution via Wasmtime

### âœ… PrÃ³s
- **Sandboxing**: Isolamento de memÃ³ria, seguranÃ§a
- **Portabilidade**: Roda em qualquer lugar (browser, server, edge)
- **Tooling**: wasm-opt, wasm-pack, debugging maduro
- **Interop**: FÃ¡cil chamar de JS/Python/etc

### âŒ Contras
- **Overhead de traduÃ§Ã£o**: VSP â†’ WASM = custo extra
- **Performance**: ~10-20% mais lento que native
- **LimitaÃ§Ãµes**: Linear memory, sem threads nativos
- **Complexidade**: Mais uma camada

### ğŸ“ Use Case
- **Ideal para**: Plugin systems, sandboxed execution, web deployment
- **Evitar**: High-performance computing, quando latÃªncia crÃ­tica

### ğŸ’» ImplementaÃ§Ã£o Estimada
```rust
use wasmtime::*;

pub struct WasmJit {
    engine: Engine,
    store: Store<()>,
}

impl WasmJit {
    pub fn compile(&mut self, bytecode: &SilcFile) -> VspResult<Module> {
        // Translate VSP â†’ WAT (WebAssembly Text)
        let wat = self.vsp_to_wat(bytecode)?;
        
        // Compile to WASM
        let module = Module::new(&self.engine, wat)?;
        Ok(module)
    }
    
    fn vsp_to_wat(&self, bytecode: &SilcFile) -> VspResult<String> {
        let mut wat = String::from("(module\n");
        wat.push_str("  (memory 1)\n");
        wat.push_str("  (func (export \"main\") (param $state i32) (result i32)\n");
        
        for inst in &bytecode.instructions {
            match inst.opcode {
                Opcode::Nop => wat.push_str("    nop\n"),
                Opcode::Add { .. } => wat.push_str("    i32.add\n"),
                // ...
            }
        }
        
        wat.push_str("    i32.const 0\n  )\n)");
        Ok(wat)
    }
}
```

**Tempo de implementaÃ§Ã£o**: 1 semana

**DependÃªncia**:
```toml
wasmtime = "26.0"
```

---

## 6. ğŸš€ **YJIT-style Lazy Basic Block Versioning**

### ğŸ’¡ Conceito
Usado pelo Ruby 3.1+. Compila blocos bÃ¡sicos sob demanda, versiona por tipo.

### âœ… PrÃ³s
- **Compilation incremental**: SÃ³ compila o que executa
- **Type specialization**: CÃ³digo diferente para Int64 vs Float
- **Memory efficient**: Blocos pequenos
- **Fast tier-up**: Interpretador â†’ JIT suave

### âŒ Contras
- **FragmentaÃ§Ã£o**: Muitas versÃµes do mesmo cÃ³digo
- **Overhead de dispatch**: Precisa escolher versÃ£o certa
- **Complexidade**: Code cache management

### ğŸ“ Use Case
- **Ideal para**: Dynamic languages, quando tipos variam
- **Evitar**: Linguagens estaticamente tipadas (como VSP)

**Nota**: Provavelmente **overkill** para VSP, pois bytecode Ã© estaticamente tipado.

---

## ğŸ“Š Tabela Comparativa

| CritÃ©rio | Cranelift JIT | LLVM JIT | Tracing JIT | DynASM | WASM |
|----------|--------------|----------|-------------|---------|------|
| **Compile Speed** | âš¡âš¡âš¡âš¡ (1-5ms) | âš¡ (50-100ms) | âš¡âš¡âš¡âš¡âš¡ (0.1ms) | âš¡âš¡âš¡âš¡âš¡ (0.1ms) | âš¡âš¡âš¡ (10ms) |
| **Runtime Speed** | âš¡âš¡âš¡ (3x) | âš¡âš¡âš¡âš¡âš¡ (10x) | âš¡âš¡âš¡âš¡ (5-10x) | âš¡âš¡âš¡âš¡âš¡ (10x) | âš¡âš¡âš¡ (5x) |
| **Memory** | 500KB | 50-100MB | 1-5MB | 50KB | 2-5MB |
| **Portabilidade** | âœ…âœ…âœ…âœ…âœ… | âœ…âœ…âœ…âœ… | âœ…âœ…âœ… | âŒ | âœ…âœ…âœ…âœ…âœ… |
| **Complexidade** | ğŸŸ¢ MÃ©dia | ğŸ”´ Alta | ğŸ”´ Muito Alta | ğŸŸ  Alta | ğŸŸ¢ MÃ©dia |
| **Debugging** | âœ… Bom | âœ…âœ… Excelente | âš ï¸ DifÃ­cil | âŒ Muito DifÃ­cil | âœ… Bom |
| **Impl. Time** | 2-3 dias | 1-2 semanas | 3-4 semanas | 2-3 semanas | 1 semana |

---

## ğŸ¯ RecomendaÃ§Ã£o para VSP

### ğŸ¥‡ **Primeira Escolha: Cranelift JIT**

**RazÃµes**:
1. âœ… **JÃ¡ temos AOT**: Reutilizar 80% do cÃ³digo
2. âœ… **Balance ideal**: Compile rÃ¡pida + performance boa
3. âœ… **Rust-native**: SeguranÃ§a de memÃ³ria, zero-cost
4. âœ… **ProduÃ§Ã£o**: Wasmtime usa em produÃ§Ã£o
5. âœ… **ImplementaÃ§Ã£o rÃ¡pida**: 2-3 dias

**Arquitetura proposta**:
```rust
// src/vsp/jit.rs
pub struct VspJit {
    module: JITModule,
    state: HashMap<String, *const u8>,
}

// Shared com AOT
fn build_function_ir(
    builder: &mut FunctionBuilder,
    bytecode: &SilcFile,
) -> VspResult<()> {
    // Mesma lÃ³gica de src/vsp/aot.rs
    // quando implementarmos compile_instructions()
}
```

### ğŸ¥ˆ **Segunda Escolha: DynASM**

Se Cranelift for muito lento (improvÃ¡vel), **DynASM** oferece compilation instantÃ¢nea.

**Quando considerar**:
- Latency < 1ms Ã© crÃ­tico
- VSP roda em hot path (milhÃµes de execuÃ§Ãµes/segundo)
- SÃ³ precisa suportar x86_64 + ARM64

### ğŸ¥‰ **Terceira Escolha: WASM (via Wasmtime)**

Se precisar de **sandboxing** ou **portabilidade mÃ¡xima** (incluindo browser).

**Use cases**:
- Plugin system (executar cÃ³digo third-party)
- Edge computing (CloudFlare Workers, etc)
- Web deployment (VSP no browser via WASM)

---

## ğŸ“ Plano de ImplementaÃ§Ã£o (Cranelift JIT)

### Fase 1: Setup (2-4 horas)
```toml
# Cargo.toml - jÃ¡ temos!
cranelift-jit = { version = "0.113", optional = true }
```

### Fase 2: Core JIT (1 dia)
```rust
// src/vsp/jit.rs
use cranelift_jit::{JITBuilder, JITModule};

pub struct VspJit {
    builder: JITBuilder,
    module: JITModule,
    compiled_functions: HashMap<String, *const u8>,
}

impl VspJit {
    pub fn new() -> VspResult<Self> {
        let mut builder = JITBuilder::new(cranelift_module::default_libcall_names())?;
        builder.hotswap(true); // Permite recompilaÃ§Ã£o
        let module = JITModule::new(builder);
        
        Ok(Self {
            builder,
            module,
            compiled_functions: HashMap::new(),
        })
    }
    
    pub fn compile(&mut self, name: &str, bytecode: SilcFile) -> VspResult<()> {
        // Reusar cÃ³digo de aot.rs
        let mut ctx = self.module.make_context();
        let mut fn_builder_ctx = FunctionBuilderContext::new();
        
        // Build function (compartilhar com AOT)
        build_vsp_function(&mut ctx, &mut fn_builder_ctx, &bytecode)?;
        
        let id = self.module.declare_function(name, Linkage::Export, &ctx.func.signature)?;
        self.module.define_function(id, &mut ctx)?;
        self.module.clear_context(&mut ctx);
        
        // Finalizar e obter ponteiro
        self.module.finalize_definitions()?;
        let code_ptr = self.module.get_finalized_function(id);
        self.compiled_functions.insert(name.to_string(), code_ptr);
        
        Ok(())
    }
    
    pub fn execute(&self, name: &str, state: &mut SilState) -> VspResult<i32> {
        let func_ptr = self.compiled_functions.get(name)
            .ok_or_else(|| VspError::Other(format!("Function {} not compiled", name)))?;
        
        let func: unsafe extern "C" fn(*mut SilState) -> i32 = unsafe {
            std::mem::transmute(*func_ptr)
        };
        
        Ok(unsafe { func(state) })
    }
}
```

### Fase 3: Shared IR Builder (1 dia)
```rust
// src/vsp/codegen.rs (novo arquivo)
// Compartilhado entre JIT e AOT

pub fn build_vsp_function(
    ctx: &mut Context,
    fn_ctx: &mut FunctionBuilderContext,
    bytecode: &SilcFile,
) -> VspResult<()> {
    let mut builder = FunctionBuilder::new(&mut ctx.func, fn_ctx);
    
    let entry = builder.create_block();
    builder.append_block_params_for_function_params(entry);
    builder.switch_to_block(entry);
    builder.seal_block(entry);
    
    // Traduzir opcodes para IR
    compile_instructions(&mut builder, bytecode)?;
    
    let zero = builder.ins().iconst(types::I32, 0);
    builder.ins().return_(&[zero]);
    builder.finalize();
    
    Ok(())
}

fn compile_instructions(
    builder: &mut FunctionBuilder,
    bytecode: &SilcFile,
) -> VspResult<()> {
    // TODO: Implementar traduÃ§Ã£o de cada opcode
    // (Mesmo cÃ³digo para JIT e AOT)
    Ok(())
}
```

### Fase 4: Benchmarks (4 horas)
```rust
// benches/jit_vs_interpreter.rs
#[bench]
fn bench_interpreter(b: &mut Bencher) { ... }

#[bench]
fn bench_jit_cold(b: &mut Bencher) { ... } // Primeira execuÃ§Ã£o

#[bench]
fn bench_jit_warm(b: &mut Bencher) { ... } // PÃ³s-compilaÃ§Ã£o
```

### Fase 5: Examples (2 horas)
```rust
// examples/vsp_jit.rs
fn main() {
    let mut jit = VspJit::new()?;
    let bytecode = SilcFile::from_file("program.silc")?;
    
    println!("â±ï¸  Compiling...");
    let start = Instant::now();
    jit.compile("main", bytecode)?;
    println!("âœ“ Compiled in {:?}", start.elapsed());
    
    println!("ğŸš€ Executing...");
    let mut state = SilState::new();
    let result = jit.execute("main", &mut state)?;
    println!("âœ“ Result: {}", result);
}
```

**Tempo total estimado**: 2-3 dias de trabalho focado

---

## ğŸ”¬ Experimentos Futuros

### Hybrid JIT/AOT
```rust
pub enum CompileMode {
    Interpret,           // Cold path
    JIT(OptLevel::None), // Warm-up
    AOT(OptLevel::Speed), // Hot path
}

// Auto tier-up baseado em contadores
```

### Multi-tier JIT (como V8)
1. **Ignition** (interpreter) â†’ cold start
2. **Sparkplug** (fast JIT, sem otimizaÃ§Ã£o) â†’ warm-up
3. **TurboFan** (optimizing JIT) â†’ hot code

Para VSP:
1. Interpreter â†’ 0ms startup
2. Cranelift JIT (O0) â†’ primeira execuÃ§Ã£o, ~1ms compile
3. Cranelift JIT (O2) â†’ hot loops, ~5ms recompile

---

## ğŸ“ ReferÃªncias

- [Cranelift JIT Tutorial](https://github.com/bytecodealliance/wasmtime/blob/main/cranelift/docs/index.md)
- [LuaJIT Tracing](http://wiki.luajit.org/SSA-IR-2.0)
- [YJIT Design](https://shopify.engineering/yjit-just-in-time-compiler-cruby)
- [DynASM Examples](https://censoredusername.github.io/dynasm-rs/language/index.html)
- [Inkwell (LLVM) Tutorial](https://github.com/TheDan64/inkwell)

---

**ConclusÃ£o**: Para VSP, **Cranelift JIT** Ã© a escolha Ã³bvia. Mesmo cÃ³digo do AOT, implementaÃ§Ã£o rÃ¡pida, performance sÃ³lida. ğŸ¯
