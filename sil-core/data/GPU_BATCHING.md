# GPU Batching ‚Äî Opera√ß√µes Ass√≠ncronas em Lote

Sistema de batching autom√°tico para opera√ß√µes GPU, otimizando throughput atrav√©s de agrupamento inteligente de computa√ß√µes.

## üéØ Objetivo

Maximizar efici√™ncia GPU atrav√©s de:
- **Batching autom√°tico**: Agrupa opera√ß√µes pequenas em batches grandes
- **Async/await**: Non-blocking, permite paralelismo de alto n√≠vel
- **Lat√™ncia controlada**: Max wait time configur√°vel
- **Throughput otimizado**: Saturar GPU com trabalho

## üìê Arquitetura

```
User Code
    ‚îÇ
    ‚îú‚îÄ‚ñ∫ compute_gradients(states) ‚îÄ‚îê
    ‚îÇ                               ‚îÇ
    ‚îú‚îÄ‚ñ∫ compute_gradients(states) ‚îÄ‚î§  Batching Queue
    ‚îÇ                               ‚îÇ  (async channel)
    ‚îú‚îÄ‚ñ∫ interpolate(a, b, t) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    ‚îÇ                               ‚îÇ
    ‚îî‚îÄ‚ñ∫ compute_gradients(states) ‚îÄ‚îò
                                    ‚îÇ
                              Batch Processor
                              (background task)
                                    ‚îÇ
                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                              ‚îÇ           ‚îÇ
                          Flush on:   Flush on:
                         Size limit  Timeout
                              ‚îÇ           ‚îÇ
                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                               GPU Dispatch
                            (wgpu compute pass)
```

## üöÄ Uso B√°sico

### 1. Setup

```rust
use sil_core::processors::gpu::{GpuContext, BatchedGpuHandle, BatchConfig};
use std::sync::Arc;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Inicializar GPU
    let ctx = GpuContext::new().await?;
    
    // Criar handle com configura√ß√£o padr√£o
    let handle = BatchedGpuHandle::new(Arc::new(ctx));
    
    // ... usar handle ...
}
```

### 2. Configura√ß√£o Customizada

```rust
let config = BatchConfig {
    max_batch_size: 1024,   // Batch at√© 1024 estados
    max_wait_ms: 5,         // Max 5ms lat√™ncia
    channel_size: 128,      // Fila com 128 ops
};

let handle = BatchedGpuHandle::with_config(Arc::new(ctx), config);
```

### 3. Computa√ß√£o de Gradientes

```rust
// Single async call
let states = vec![SilState::from_byte(0x42); 100];
let gradients = handle.compute_gradients(states).await?;

// Parallel batching
let mut tasks = Vec::new();
for chunk in all_states.chunks(100) {
    let handle = handle.clone();
    let chunk = chunk.to_vec();
    
    let task = tokio::spawn(async move {
        handle.compute_gradients(chunk).await
    });
    
    tasks.push(task);
}

// Collect results
for task in tasks {
    let grads = task.await??;
    process_gradients(grads);
}
```

### 4. Interpola√ß√£o

```rust
// Interpolar entre dois conjuntos de estados
let interpolated = handle.interpolate(
    states_a,
    states_b,
    0.5,        // t = 50%
    true,       // use slerp (spherical interpolation)
).await?;

// Criar anima√ß√£o de 10 frames
for i in 0..10 {
    let t = i as f32 / 9.0;
    let frame = handle.interpolate(
        vec![start_state],
        vec![end_state],
        t,
        false, // use lerp (linear)
    ).await?;
    
    render_frame(&frame[0]);
}
```

## ‚öôÔ∏è Configura√ß√£o

### BatchConfig

| Campo | Default | Descri√ß√£o |
|-------|---------|-----------|
| `max_batch_size` | 1024 | N√∫mero m√°ximo de estados por batch |
| `max_wait_ms` | 5 | Lat√™ncia m√°xima antes de flush (ms) |
| `channel_size` | 128 | Tamanho da fila de opera√ß√µes |

### Trade-offs

**Batch size grande:**
- ‚úÖ Maior throughput GPU
- ‚úÖ Menos overhead de dispatch
- ‚ùå Maior lat√™ncia individual
- ‚ùå Mais mem√≥ria GPU

**Max wait curto:**
- ‚úÖ Menor lat√™ncia
- ‚ùå Batches menores
- ‚ùå Menor throughput

**Recomenda√ß√µes:**
- Workloads latency-sensitive: `max_batch_size=256, max_wait_ms=1`
- Workloads throughput-intensive: `max_batch_size=2048, max_wait_ms=10`
- Balanced: `max_batch_size=1024, max_wait_ms=5` (default)

## üî¨ Performance

### Exemplo: 10K estados

```rust
// Sem batching (blocking sync)
for state in states {
    let grad = compute_gradient_sync(state);  // ~100¬µs/estado
}
// Total: ~1000ms

// Com batching (async)
let grads = handle.compute_gradients(states).await?;
// Total: ~50ms (20x speedup)
```

### Throughput Esperado

| GPU | Estados/segundo | Notas |
|-----|-----------------|-------|
| M3 Pro | ~200K | Metal backend |
| RTX 3080 | ~500K | Vulkan/CUDA |
| RTX 4090 | ~1M | Top-tier consumer |

*Nota: Depende de complexidade do shader e tamanho do batch*

## üõ†Ô∏è Implementa√ß√£o

### Estrutura Interna

```rust
pub struct BatchedGpuExecutor {
    ctx: Arc<GpuContext>,           // GPU context compartilhado
    tx: mpsc::Sender<GpuOp>,        // Canal para submeter ops
    config: BatchConfig,            // Configura√ß√£o de batching
}

pub enum GpuOp {
    ComputeGradients {
        states: Vec<SilState>,
        response: oneshot::Sender<Result<Vec<SilGradient>>>,
    },
    // ... outras ops ...
}
```

### Background Processor

```rust
async fn batch_processor(
    ctx: Arc<GpuContext>,
    mut rx: mpsc::Receiver<GpuOp>,
    config: BatchConfig,
) {
    let mut batch = GpuBatch::new();
    
    loop {
        // Receber com timeout
        match timeout(Duration::from_millis(config.max_wait_ms), rx.recv()).await {
            Ok(Some(op)) => {
                batch.add(op);
                
                // Flush se cheio
                if batch.should_flush(config.max_batch_size) {
                    execute_batch(&ctx, batch).await;
                    batch = GpuBatch::new();
                }
            }
            Err(_) => {
                // Timeout - flush pendente
                if !batch.is_empty() {
                    execute_batch(&ctx, batch).await;
                    batch = GpuBatch::new();
                }
            }
            Ok(None) => break, // Canal fechado
        }
    }
}
```

## üêõ Troubleshooting

### "GPU n√£o dispon√≠vel"

Verifique que a feature `gpu` est√° ativada:
```toml
sil-core = { version = "2026.1", features = ["gpu"] }
```

### "Timeout na execu√ß√£o GPU"

1. Aumentar `channel_size` se fila est√° cheia
2. Verificar se GPU n√£o est√° sobrecarregada
3. Reduzir `max_batch_size` se mem√≥ria insuficiente

### Performance baixa

1. Aumentar `max_batch_size` para saturar GPU
2. Usar `tokio::spawn` para paralelizar submiss√µes
3. Verificar que n√£o est√° CPU-bound (profile com `cargo flamegraph`)

## üìä Benchmarks

```bash
cargo bench --features gpu -- batching
```

Compara:
- Sync blocking vs async batching
- Diferentes tamanhos de batch
- Single-threaded vs multi-threaded submission

## üîÆ Roadmap

- [ ] Implementar interpola√ß√£o GPU batched
- [ ] Pool de buffers para reduzir allocations
- [ ] M√©tricas de utiliza√ß√£o (batch fill rate, wait time)
- [ ] Auto-tuning de configura√ß√£o baseado em workload
- [ ] Multi-GPU load balancing
- [ ] Stream processing para datasets grandes

## üìö Ver Tamb√©m

- [GPU Context](GPU_CONTEXT.md) - Inicializa√ß√£o wgpu
- [Shader Pre-compilation](SHADER_PRECOMPILATION.md) - Build-time shaders
- [Performance Guide](PERFORMANCE_GUIDE.md) - Otimiza√ß√£o geral

---

**Autor**: SIL-Team  
**Vers√£o**: 2026.1.0  
**Status**: ‚úÖ Totalmente Implementado
