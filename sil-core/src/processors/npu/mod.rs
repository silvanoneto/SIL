//! # ğŸ§  NPU â€” Neural Processing Unit
//!
//! AceleraÃ§Ã£o de inferÃªncia neural via backends nativos:
//! - **macOS/iOS**: Core ML (Apple Neural Engine)
//! - **Android**: NNAPI (Neural Networks API)
//! - **Windows**: DirectML
//! - **Linux**: OpenVINO / TensorRT
//!
//! ## Conceito
//!
//! NPU Ã© otimizado para operaÃ§Ãµes de inferÃªncia:
//! - QuantizaÃ§Ã£o INT8/FP16
//! - OperaÃ§Ãµes de convoluÃ§Ã£o
//! - Transformers e atenÃ§Ã£o
//! - Batch processing eficiente
//!
//! ## Arquitetura SIL-NPU
//!
//! ```text
//! â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
//! â”‚                    SilState                         â”‚
//! â”‚  16 camadas Ã— (Ï: i4, Î¸: u4) = 128 bits            â”‚
//! â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
//!                      â”‚ Quantize
//!                      â–¼
//! â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
//! â”‚              NPU Input Tensor                       â”‚
//! â”‚  [batch, 16, 2] float16 ou int8                    â”‚
//! â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
//!                      â”‚ Inference
//!                      â–¼
//! â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
//! â”‚              NPU Output Tensor                      â”‚
//! â”‚  ClassificaÃ§Ã£o, Embedding, PrediÃ§Ã£o                â”‚
//! â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
//! ```
//!
//! ## Uso
//!
//! ```ignore
//! use sil_core::processors::npu::{NpuContext, NpuModel};
//!
//! let npu = NpuContext::new()?;
//! let model = npu.load_model("sil_classifier.mlmodel")?;
//! let result = npu.infer(&model, &state)?;
//! ```

mod context;
mod model;
mod tensor;
mod backends;

pub use context::{NpuContext, Precision};
pub use model::{NpuModel, ModelFormat};
pub use tensor::{NpuTensor, TensorLayout, DataType};

/// Erro de NPU
#[derive(Debug, thiserror::Error)]
pub enum NpuError {
    #[error("NPU nÃ£o disponÃ­vel")]
    NotAvailable,
    
    #[error("Backend nÃ£o suportado: {0}")]
    UnsupportedBackend(String),
    
    #[error("Modelo invÃ¡lido: {0}")]
    InvalidModel(String),
    
    #[error("Formato nÃ£o suportado: {0}")]
    UnsupportedFormat(String),
    
    #[error("Erro de inferÃªncia: {0}")]
    InferenceError(String),
    
    #[error("Erro de quantizaÃ§Ã£o: {0}")]
    QuantizationError(String),
    
    #[error("Tamanho de tensor invÃ¡lido: esperado {expected}, recebido {actual}")]
    TensorSizeMismatch { expected: usize, actual: usize },
    
    #[error("IO error: {0}")]
    Io(#[from] std::io::Error),
}

/// Resultado NPU
pub type NpuResult<T> = Result<T, NpuError>;

/// Resultado de inferÃªncia
#[derive(Debug, Clone)]
pub struct InferenceResult {
    /// Output tensor
    pub output: NpuTensor,
    /// Tempo de inferÃªncia em microsegundos
    pub latency_us: u64,
    /// Backend utilizado
    pub backend: NpuBackend,
}

/// Backend NPU
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum NpuBackend {
    /// Apple Neural Engine (via Core ML)
    CoreML,
    /// Android Neural Networks API
    NNAPI,
    /// Microsoft DirectML
    DirectML,
    /// Intel OpenVINO
    OpenVINO,
    /// NVIDIA TensorRT
    TensorRT,
    /// CPU Fallback (usando SIMD)
    CpuFallback,
}

impl NpuBackend {
    /// Detecta o melhor backend disponÃ­vel
    pub fn detect() -> Self {
        #[cfg(target_os = "macos")]
        {
            return Self::CoreML;
        }
        
        #[cfg(target_os = "ios")]
        {
            return Self::CoreML;
        }
        
        #[cfg(target_os = "android")]
        {
            return Self::NNAPI;
        }
        
        #[cfg(target_os = "windows")]
        {
            return Self::DirectML;
        }
        
        #[cfg(target_os = "linux")]
        {
            // Tenta OpenVINO primeiro, depois TensorRT
            return Self::OpenVINO;
        }
        
        #[allow(unreachable_code)]
        Self::CpuFallback
    }
    
    /// Nome do backend
    pub fn name(&self) -> &'static str {
        match self {
            Self::CoreML => "Core ML",
            Self::NNAPI => "NNAPI",
            Self::DirectML => "DirectML",
            Self::OpenVINO => "OpenVINO",
            Self::TensorRT => "TensorRT",
            Self::CpuFallback => "CPU Fallback",
        }
    }
}
