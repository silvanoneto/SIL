// Paebiru ML Library - Transformer Blocks
// Encoder e decoder blocks

use core::bytesil::{bs_mag, bs_from_mag_clamped};
use core::state::{st_add};
use layers::norm::{rms_norm};
use arch::attention::{matmul_4x4, scaled_dot_attention_4x4};
use arch::ffn::{swiglu_ffn, mlp_2layer};

// =============================================================================
// Transformer Encoder Block (GPT-style, Pre-LayerNorm)
// =============================================================================

// x = x + Attention(Norm(x))
// x = x + FFN(Norm(x))
pub fn transformer_encoder_block(
    input: State,
    norm1_gamma: State,
    wq: State, wk: State, wv: State, wo: State,
    norm2_gamma: State,
    ffn_w_gate: State, ffn_w_up: State, ffn_w_down: State,
    eps: Float
) -> State {
    // Self-attention with pre-norm
    let normed1 = rms_norm(input, norm1_gamma, eps);

    let q = matmul_4x4(wq, normed1);
    let k = matmul_4x4(wk, normed1);
    let v = matmul_4x4(wv, normed1);

    let attn_out = scaled_dot_attention_4x4(q, k, v);
    let attn_proj = matmul_4x4(wo, attn_out);

    let x = st_add(input, attn_proj);

    // FFN with pre-norm
    let normed2 = rms_norm(x, norm2_gamma, eps);
    let ffn_out = swiglu_ffn(normed2, ffn_w_gate, ffn_w_up, ffn_w_down);

    return st_add(x, ffn_out);
}

// =============================================================================
// Transformer Encoder Block (BERT-style, Post-LayerNorm)
// =============================================================================

// x = Norm(x + Attention(x))
// x = Norm(x + FFN(x))
pub fn transformer_encoder_block_post(
    input: State,
    norm1_gamma: State,
    wq: State, wk: State, wv: State, wo: State,
    norm2_gamma: State,
    ffn_w_gate: State, ffn_w_up: State, ffn_w_down: State,
    eps: Float
) -> State {
    // Self-attention
    let q = matmul_4x4(wq, input);
    let k = matmul_4x4(wk, input);
    let v = matmul_4x4(wv, input);

    let attn_out = scaled_dot_attention_4x4(q, k, v);
    let attn_proj = matmul_4x4(wo, attn_out);

    let x = rms_norm(st_add(input, attn_proj), norm1_gamma, eps);

    // FFN
    let ffn_out = swiglu_ffn(x, ffn_w_gate, ffn_w_up, ffn_w_down);

    return rms_norm(st_add(x, ffn_out), norm2_gamma, eps);
}

// =============================================================================
// Transformer Decoder Block (with Cross-Attention)
// =============================================================================

// x = x + SelfAttn(Norm(x))
// x = x + CrossAttn(Norm(x), encoder_output)
// x = x + FFN(Norm(x))
pub fn transformer_decoder_block(
    input: State,
    encoder_output: State,
    norm1_gamma: State,
    wq_self: State, wk_self: State, wv_self: State, wo_self: State,
    norm2_gamma: State,
    wq_cross: State, wk_cross: State, wv_cross: State, wo_cross: State,
    norm3_gamma: State,
    ffn_w_gate: State, ffn_w_up: State, ffn_w_down: State,
    eps: Float
) -> State {
    // Self-attention
    let normed1 = rms_norm(input, norm1_gamma, eps);
    let q_self = matmul_4x4(wq_self, normed1);
    let k_self = matmul_4x4(wk_self, normed1);
    let v_self = matmul_4x4(wv_self, normed1);
    let self_attn = scaled_dot_attention_4x4(q_self, k_self, v_self);
    let self_attn_proj = matmul_4x4(wo_self, self_attn);
    let x = st_add(input, self_attn_proj);

    // Cross-attention
    let normed2 = rms_norm(x, norm2_gamma, eps);
    let q_cross = matmul_4x4(wq_cross, normed2);
    let k_cross = matmul_4x4(wk_cross, encoder_output);
    let v_cross = matmul_4x4(wv_cross, encoder_output);
    let cross_attn = scaled_dot_attention_4x4(q_cross, k_cross, v_cross);
    let cross_attn_proj = matmul_4x4(wo_cross, cross_attn);
    let x = st_add(x, cross_attn_proj);

    // FFN
    let normed3 = rms_norm(x, norm3_gamma, eps);
    let ffn_out = swiglu_ffn(normed3, ffn_w_gate, ffn_w_up, ffn_w_down);

    return st_add(x, ffn_out);
}

// =============================================================================
// Simplified Blocks
// =============================================================================

// Simplified encoder block without output projection
pub fn transformer_encoder_simple(
    input: State,
    norm_gamma: State,
    wq: State, wk: State, wv: State,
    ffn_w1: State, ffn_w2: State,
    eps: Float
) -> State {
    // Self-attention
    let normed = rms_norm(input, norm_gamma, eps);
    let q = matmul_4x4(wq, normed);
    let k = matmul_4x4(wk, normed);
    let v = matmul_4x4(wv, normed);
    let attn_out = scaled_dot_attention_4x4(q, k, v);
    let x = st_add(input, attn_out);

    // Simple 2-layer FFN
    let normed2 = rms_norm(x, norm_gamma, eps);
    let ffn_out = mlp_2layer(normed2, ffn_w1, ffn_w2);

    return st_add(x, ffn_out);
}

// =============================================================================
// Final Normalization
// =============================================================================

// Final layer normalization (for output)
pub fn final_norm(input: State, gamma: State, eps: Float) -> State {
    return rms_norm(input, gamma, eps);
}

