// Paebiru ML Library - Optimizers
// Algoritmos de otimizacao para treinamento de redes neurais

use core::state::{st_add, st_sub, st_scale, st_mul, st_norm_l2};
use core::bytesil::{bs_mag};

// =============================================================================
// SGD (Stochastic Gradient Descent)
// =============================================================================

// SGD basico: params = params - lr * gradients
pub fn sgd_step(params: State, gradients: State, lr: Float) -> State {
    let scaled_grad = st_scale(gradients, lr);
    return st_sub(params, scaled_grad);
}

// SGD com momentum
// velocity_new = momentum * velocity - lr * gradients
// params_new = params + velocity_new
pub fn sgd_momentum_velocity(velocity: State, gradients: State, lr: Float, momentum: Float) -> State {
    let v_momentum = st_scale(velocity, momentum);
    let grad_scaled = st_scale(gradients, lr);
    return st_sub(v_momentum, grad_scaled);
}

pub fn sgd_momentum_step(params: State, velocity: State) -> State {
    return st_add(params, velocity);
}

// =============================================================================
// Gradient Clipping
// =============================================================================

pub fn clip_grad_norm(gradients: State, max_norm: Float) -> State {
    let norm = st_norm_l2(gradients);
    if norm > max_norm {
        return st_scale(gradients, max_norm / norm);
    }
    return gradients;
}

pub fn clip_grad_value(gradients: State, clip_value: Float) -> State {
    let r = state_vacuum();
    let m0 = clamp_float(bs_mag(state_get_layer(gradients, 0)), 0.0 - clip_value, clip_value);
    let r = state_set_layer(r, 0, bytesil_from_complex(m0, 0.0));
    let m1 = clamp_float(bs_mag(state_get_layer(gradients, 1)), 0.0 - clip_value, clip_value);
    let r = state_set_layer(r, 1, bytesil_from_complex(m1, 0.0));
    let m2 = clamp_float(bs_mag(state_get_layer(gradients, 2)), 0.0 - clip_value, clip_value);
    let r = state_set_layer(r, 2, bytesil_from_complex(m2, 0.0));
    let m3 = clamp_float(bs_mag(state_get_layer(gradients, 3)), 0.0 - clip_value, clip_value);
    let r = state_set_layer(r, 3, bytesil_from_complex(m3, 0.0));
    let m4 = clamp_float(bs_mag(state_get_layer(gradients, 4)), 0.0 - clip_value, clip_value);
    let r = state_set_layer(r, 4, bytesil_from_complex(m4, 0.0));
    let m5 = clamp_float(bs_mag(state_get_layer(gradients, 5)), 0.0 - clip_value, clip_value);
    let r = state_set_layer(r, 5, bytesil_from_complex(m5, 0.0));
    let m6 = clamp_float(bs_mag(state_get_layer(gradients, 6)), 0.0 - clip_value, clip_value);
    let r = state_set_layer(r, 6, bytesil_from_complex(m6, 0.0));
    let m7 = clamp_float(bs_mag(state_get_layer(gradients, 7)), 0.0 - clip_value, clip_value);
    let r = state_set_layer(r, 7, bytesil_from_complex(m7, 0.0));
    let m8 = clamp_float(bs_mag(state_get_layer(gradients, 8)), 0.0 - clip_value, clip_value);
    let r = state_set_layer(r, 8, bytesil_from_complex(m8, 0.0));
    let m9 = clamp_float(bs_mag(state_get_layer(gradients, 9)), 0.0 - clip_value, clip_value);
    let r = state_set_layer(r, 9, bytesil_from_complex(m9, 0.0));
    let m10 = clamp_float(bs_mag(state_get_layer(gradients, 10)), 0.0 - clip_value, clip_value);
    let r = state_set_layer(r, 10, bytesil_from_complex(m10, 0.0));
    let m11 = clamp_float(bs_mag(state_get_layer(gradients, 11)), 0.0 - clip_value, clip_value);
    let r = state_set_layer(r, 11, bytesil_from_complex(m11, 0.0));
    let m12 = clamp_float(bs_mag(state_get_layer(gradients, 12)), 0.0 - clip_value, clip_value);
    let r = state_set_layer(r, 12, bytesil_from_complex(m12, 0.0));
    let m13 = clamp_float(bs_mag(state_get_layer(gradients, 13)), 0.0 - clip_value, clip_value);
    let r = state_set_layer(r, 13, bytesil_from_complex(m13, 0.0));
    let m14 = clamp_float(bs_mag(state_get_layer(gradients, 14)), 0.0 - clip_value, clip_value);
    let r = state_set_layer(r, 14, bytesil_from_complex(m14, 0.0));
    let m15 = clamp_float(bs_mag(state_get_layer(gradients, 15)), 0.0 - clip_value, clip_value);
    let r = state_set_layer(r, 15, bytesil_from_complex(m15, 0.0));
    return r;
}

// =============================================================================
// Learning Rate Schedules
// =============================================================================

pub fn lr_constant(initial_lr: Float) -> Float {
    return initial_lr;
}

pub fn lr_cosine_annealing(initial_lr: Float, step: Int, total_steps: Int, min_lr: Float) -> Float {
    let progress = step * 1.0 / total_steps * 1.0;
    let cos_val = cos(pi() * progress);
    return min_lr + 0.5 * (initial_lr - min_lr) * (1.0 + cos_val);
}

pub fn lr_warmup(target_lr: Float, step: Int, warmup_steps: Int) -> Float {
    if step >= warmup_steps {
        return target_lr;
    }
    return target_lr * (step * 1.0 / warmup_steps * 1.0);
}

pub fn lr_step_decay(initial_lr: Float, step: Int, step_size: Int, gamma: Float) -> Float {
    let num_decays = step / step_size;
    return initial_lr * pow_float(gamma, num_decays * 1.0);
}

pub fn lr_exponential_decay(initial_lr: Float, step: Int, decay_rate: Float) -> Float {
    return initial_lr * exp(0.0 - decay_rate * step * 1.0);
}

pub fn lr_warmup_cosine(initial_lr: Float, step: Int, warmup_steps: Int, total_steps: Int, min_lr: Float) -> Float {
    if step < warmup_steps {
        return lr_warmup(initial_lr, step, warmup_steps);
    }
    let adjusted_step = step - warmup_steps;
    let adjusted_total = total_steps - warmup_steps;
    return lr_cosine_annealing(initial_lr, adjusted_step, adjusted_total, min_lr);
}

// =============================================================================
// Weight Initialization Scales
// =============================================================================

pub fn xavier_scale(fan_in: Int, fan_out: Int) -> Float {
    return sqrt(6.0 / (fan_in * 1.0 + fan_out * 1.0));
}

pub fn he_scale(fan_in: Int) -> Float {
    return sqrt(2.0 / (fan_in * 1.0));
}

pub fn lecun_scale(fan_in: Int) -> Float {
    return sqrt(1.0 / (fan_in * 1.0));
}
